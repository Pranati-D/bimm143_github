---
title: "Class 07: machine Learning 1"
author: Pranati
format: pdf
---

Today we will start out multi-part exploration of some key machine learning methods. We will begin with clustering - finding groupings in data, and then dimensionallity reduction.


## Clustering

Let's start with "k-means" clustering.
The main function in base R for this `kmeans()`.

```{r}
# Make up some data
hist(rnorm(100000, mean=3))
```


```{r}
tmp <- c(rnorm(30, -3), rnorm (30, +3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Now let's try out `kmeans()`

```{r}
km <- kmeans(x, centers=2)
km
```

>Q. How many points in each cluster?

```{r}
km$size
```

>Q. What component of your result object details cluster assignment/membership?

```{r}
km$cluster
```


>Q. What are centers/mean values of each cluster?

```{r}
km$centers
```


>Q. Make a plot of your data showing your clustering results (groupings/clusters and cluster centers).

```{r}
plot(x, col = c(1, 2))
```


```{r}
plot(x, col=km$cluster)
points(km$centers, col="green", pch=15, cex=3)
```

>Q. Run `kmeans()` again and cluster in 4 groups and plot the results.

```{r}
km4 <- kmeans(x, centers=4)
km4
```

```{r}
plot(x, col=km4$cluster)
```

## Hierarchical Clustering

This form of clustering aims to reveal the structure in your data by progressivey grouping points into a ever smaller number of clusters. 

The main function in base R for this called `hclust()`. This function does not take our input data directly but wants a "distance matrix" that details how (dis)similar all our input points are to each other.

```{r}
dist(x)
```

```{r}
hc <- hclust(dist(x))
hc
```

The print out above is not very useful (unlike that from kmeans) but there is a useful `plot()` method.

```{r}
plot(hc)
```

```{r}
plot(hc)
abline(h=10, col="red")
```


To gte my main result (my cluster membership vector) I need to "cut" my tree using the function `cutree()`

```{r}
grps <- cutree(hc, h=10)
grps
```

```{r}
plot(x, col = grps)
```




# Principal Component Analysis (PCA)

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

>Q1.

```{r}
dim(x)
```

```{r}
head(x,6)
```
```{r}
##rownames(x) <- x[,1]
##x <- x[,-1]
##head(x)
```

>Q2. 
I prefer the second option because if I run the above code again, it will delete the name to the next column.

```{r}
x <- read.csv(url, row.names=1)
head(x)
```

>Q3.
Change beside=T to beside=F.

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

The so-called "pairs" plot can be useful for small datasets: 

```{r}
#rainbow(nrow(x))
pairs(x, col=rainbow(nrow(x)),pch=16)
```

>Q5.
The graph shows duplicate information, for example the plot next to the England box has England on the x axis and Wales on the y. While the plot directly below the England box has Wales on the x axis and England on the y. As a result, the two plots are the same, just flipped cause of the flipped axes.

>Q6.
Ireland seems to be to have more similar graphs across. But we can use PCA to clarify all the differences.

The pairs plot is useful for small datasets but it can be lots of work to interpret and gets untractable for larger datasets.

So PCA to the rescue...

The main function to do PCA is base R is called `prcomp()`.This function wants the trasnpose of our data in this case.

```{r}
pca <- prcomp(t(x))
summary(pca)
```
```{r}
attributes(pca)
```
```{r}
pca$x
```
a major PCA result viz is called a "PCA plot" (a.k.a. a score plot, biplot, PC1 vs PC2 plot, ordination plot)

```{r}
mycols <- c("orange", "red", "blue", "darkgreen")
plot(pca$x[,1], pca$x[,2], col=mycols, pch=16,
     xlab="PC1", ylab="PC2")
abline(h=0, col="gray")
abline(v=0, col="gray")
```

Another important output from PCA is called the "loadings" vector or the "rotation" component - this tells us how much the original variables (the foods in this case) contribute to the new PCs.

```{r}
pca$rotation
```

PCA looks to be super useful method for gaining some insight into high dimensional data that is difficult to examine in other ways.

# PCA of RNASeq Data

```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```

```{r}
# Again we have to transpose our data
pca <- prcomp(t(rna.data), scale=TRUE)
```

```{r}
summary(pca)
```

>Q10. How many genes in the dataset?

```{r}
nrow(rna.data)
```

```{r}
attributes(pca)
```

```{r}
pca$x
```

```{r}
head(pca$x)
```

I will make a main result figure using ggplot:

```{r}
library(ggplot2)
```

```{r}
res <- as.data.frame(pca$x)
```

```{r}
head(res)
```

```{r}
ggplot(res) +
  aes(PC1, PC2) +
  geom_point()
```



```{r}
mycols <- c(rep("blue", 5), rep("red", 5))
mycols
```

```{r}
ggplot(res) + 
  aes(x=PC1, y=PC2, label=row.names(res)) +
  geom_point(col=mycols) +
  geom_label(col=mycols)
```

```{r}
colnames(rna.data)
```

```{r}
kmeans(pca$x[,1], centers = 2)
```




